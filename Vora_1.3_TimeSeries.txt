#################################
# SAP HANA VORA 1.3 TIME SERIES #
#################################

clear
rm adv_sale.csv
hdfs dfs -rm /user/cluster_admin/adv_sale.csv

echo 'TS,ADV,SALE' >> adv_sale.csv
echo '2014-01-01 00:00:00.0,12.0,15.0' >> adv_sale.csv
echo '2014-02-01 00:00:00.0,20.5,16.0' >> adv_sale.csv
echo '2014-03-01 00:00:00.0,21.0,18.0' >> adv_sale.csv
echo '2014-04-01 00:00:00.0,15.5,27.0' >> adv_sale.csv
echo '2014-05-01 00:00:00.0,15.3,21.0' >> adv_sale.csv
echo '2014-06-01 00:00:00.0,23.5,49.0' >> adv_sale.csv
echo '2014-07-01 00:00:00.0,24.5,21.0' >> adv_sale.csv
echo '2014-08-01 00:00:00.0,21.3,22.0' >> adv_sale.csv
echo '2014-09-01 00:00:00.0,23.5,28.0' >> adv_sale.csv
echo '2014-10-01 00:00:00.0,28.0,36.0' >> adv_sale.csv
echo '2014-11-01 00:00:00.0,24.0,40.0' >> adv_sale.csv
echo '2014-12-01 00:00:00.0,15.5,3.0' >> adv_sale.csv
echo '2015-01-01 00:00:00.0,17.3,21.0' >> adv_sale.csv
echo '2015-02-01 00:00:00.0,25.3,29.0' >> adv_sale.csv
echo '2015-03-01 00:00:00.0,25.0,62.0' >> adv_sale.csv
echo '2015-04-01 00:00:00.0,36.5,65.0' >> adv_sale.csv
echo '2015-05-01 00:00:00.0,36.5,46.0' >> adv_sale.csv
echo '2015-06-01 00:00:00.0,29.6,44.0' >> adv_sale.csv
echo '2015-07-01 00:00:00.0,30.5,33.0' >> adv_sale.csv
echo '2015-08-01 00:00:00.0,28.0,62.0' >> adv_sale.csv
echo '2015-09-01 00:00:00.0,26.0,22.0' >> adv_sale.csv
echo '2015-10-01 00:00:00.0,21.5,12.0' >> adv_sale.csv
echo '2015-11-01 00:00:00.0,19.7,24.0' >> adv_sale.csv
echo '2015-12-01 00:00:00.0,19.0,3.0' >> adv_sale.csv
echo '2016-01-01 00:00:00.0,16.0,5.0' >> adv_sale.csv
echo '2016-02-01 00:00:00.0,20.7,14.0' >> adv_sale.csv
echo '2016-03-01 00:00:00.0,26.5,36.0' >> adv_sale.csv
echo '2016-04-01 00:00:00.0,30.6,40.0' >> adv_sale.csv
echo '2016-05-01 00:00:00.0,32.3,49.0' >> adv_sale.csv
echo '2016-06-01 00:00:00.0,29.5,7.0' >> adv_sale.csv
echo '2016-07-01 00:00:00.0,28.3,52.0' >> adv_sale.csv
echo '2016-08-01 00:00:00.0,31.3,65.0' >> adv_sale.csv
echo '2016-09-01 00:00:00.0,32.2,17.0' >> adv_sale.csv
echo '2016-10-01 00:00:00.0,26.4,5.0' >> adv_sale.csv
echo '2016-11-01 00:00:00.0,23.4,17.0' >> adv_sale.csv
echo '2016-12-01 00:00:00.0,16.4,1.0' >> adv_sale.csv

hdfs dfs -put adv_sale.csv /user/cluster_admin
hdfs dfs -cat /user/cluster_admin/adv_sale.csv

DROP PARTITION SCHEME PS_TS USING com.sap.spark.engines;
DROP PARTITION FUNCTION PF_TS USING com.sap.spark.engines;

CREATE PARTITION FUNCTION PF_TS(TS TIMESTAMP) AS RANGE 
BOUNDARIES(TIMESTAMP '2014-01-01 00:00:00', TIMESTAMP '2016-12-01 00:00:00') 
USING com.sap.spark.engines;

CREATE PARTITION SCHEME PS_TS USING PF_TS USING com.sap.spark.engines;

CREATE TABLE ADV_SALE (TS TIMESTAMP, ADV double, SALE double) 
SERIES 
(
PERIOD FOR SERIES TS 
START TIMESTAMP '2014-01-01 00:00:00' 
END TIMESTAMP '2016-12-01 00:00:00'
) 
PARTITION BY PS_TS(TS) USING com.sap.spark.engines 
OPTIONS (files "/user/cluster_admin/adv_sale.csv",
csvskip "1");

``SELECT * FROM ADV_SALE`` USING com.sap.spark.engines;

``SELECT TS, ADV FROM ADV_SALE 
WHERE PERIOD AS OF TIMESTAMP '2015-09-01 00:00:00'`` 
USING com.sap.spark.engines;

# Simple analysis: Sale trend over the last 4 months in 2016
``SELECT TREND(SALE) as SALETREND 
FROM ADV_SALE 
WHERE PERIOD AS OF TIMESTAMP '2016-08-01 00:00:00'`` 
USING com.sap.spark.engines;

# Multi column analysis with Sales-Advertising
``SELECT median(SALE) as SALEMEDIAN, median(ADV) as ADVMEDIAN
FROM ADV_SALE 
WHERE PERIOD BETWEEN TIMESTAMP '2015-08-01 00:00:00' 
AND TIMESTAMP '2016-08-01 00:00:00'`` 
USING com.sap.spark.engines;

# What is the overall trend in available sales and advertising over the year of 2015?
``SELECT TREND(SALE) as SALETREND, TREND(ADV) AS ADVTREND 
FROM ADV_SALE 
WHERE PERIOD BETWEEN TIMESTAMP '2015-01-01 00:00:00' 
AND TIMESTAMP '2016-01-01 00:00:00'`` 
USING com.sap.spark.engines; 

``select max(SALE) AS MAXSALE 
from ADV_SALE`` 
USING com.sap.spark.engines;

``select mode(SALE) AS MODESALE, mode(ADV) AS MODEADV 
from ADV_SALE`` 
USING com.sap.spark.engines;

